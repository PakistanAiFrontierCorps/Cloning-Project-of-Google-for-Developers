<!DOCTYPE html><html lang="en" dir="ltr"><head>
    <title>Google for Developers</title>
    <meta name="robots" content="noindex,indexifembedded">
    <meta charset="utf-8">
    <meta content="IE=Edge" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="iframe" content="standalone">
    
    
    <script nonce="rrZRv/SYJuZ3ueQ1DeL0tfoXrsTI1z">
      (function(){
        window.framebox=window.framebox||function(){(window.framebox.q=window.framebox.q||[]).push(arguments)};
        
        var a={},b=function(){(window.framebox.dq=window.framebox.dq||[]).push(arguments)};
        ['getUrl','handleLinkClicksInParent','initAutoSize','navigate','pushState','replaceState',
         'requestQueryAndFragment','sendEvent','updateSize','scrollParentWindow']
          .forEach(function(x){a[x]=function(){b(x,arguments)}});
        window.devsite={framebox:{AutoSizeClient:a}};
      })();
      
      (function(d,e,v,s,i,t,E){d['GoogleDevelopersObject']=i;
        t=e.createElement(v);t.async=1;t.src=s;E=e.getElementsByTagName(v)[0];
        E.parentNode.insertBefore(t,E);})(window, document, 'script',
        'https://www.gstatic.com/devrel-devsite/prod/v80280542cfb431993d6ccf12e26a1c5862cffb314c3cfae3ff08e8374a93b7f7/developers/js/app_loader.js', '[1,"en",null,"/js/devsite_app_module.js","https://www.gstatic.com/devrel-devsite/prod/v80280542cfb431993d6ccf12e26a1c5862cffb314c3cfae3ff08e8374a93b7f7","https://www.gstatic.com/devrel-devsite/prod/v80280542cfb431993d6ccf12e26a1c5862cffb314c3cfae3ff08e8374a93b7f7/developers","https://developers-dot-devsite-v2-prod.appspot.com",1,1,null,1,null,[1,6,8,12,14,17,21,25,50,52,63,70,75,76,80,87,91,92,93,97,98,100,101,102,103,104,105,107,108,109,110,112,113,116,117,118,120,122,124,125,126,127,129,130,131,132,133,134,135,136,138,140,141,147,148,149,151,152,156,157,158,159,161,163,164,168,169,170,179,180,182,183,186,191,193,196],"AIzaSyAP-jjEJBzmIyKR4F-3XITp8yM9T1gEEI8","AIzaSyB6xiKGDR5O3Ak2okS4rLkauxGUG7XP0hg","developers.google.com","AIzaSyAQk0fBONSGUqCNznf6Krs82Ap1-NV6J4o","AIzaSyCCxcqdrZ_7QMeLCRY20bh_SXdAYqy70KY",null,null,null,["Concierge__enable_pushui","Concierge__enable_concierge_restricted","Search__enable_dynamic_content_confidential_banner","Profiles__enable_awarding_url","MiscFeatureFlags__emergency_css","Cloud__enable_cloud_dlp_service","Cloud__enable_cloudx_ping","Significatio__enable_by_tenant","MiscFeatureFlags__enable_variable_operator","Profiles__enable_recognition_badges","Profiles__enable_dashboard_curated_recommendations","Search__enable_ai_eligibility_checks","MiscFeatureFlags__developers_footer_dark_image","Cloud__enable_cloud_shell","TpcFeatures__enable_mirror_tenant_redirects","Cloud__enable_llm_concierge_chat","Search__enable_suggestions_from_borg","Profiles__enable_public_developer_profiles","Experiments__reqs_query_experiments","Profiles__enable_release_notes_notifications","Profiles__enable_developer_profiles_callout","Search__enable_page_map","Cloud__enable_cloud_facet_chat","Profiles__enable_page_saving","MiscFeatureFlags__enable_project_variables","CloudShell__cloud_code_overflow_menu","Cloud__enable_cloudx_experiment_ids","CloudShell__cloud_shell_button","MiscFeatureFlags__developers_footer_image","Analytics__enable_cookie_bar","Profiles__require_profile_eligibility_for_signin","Search__enable_ai_search_summaries","Search__enable_ai_search_summaries_restricted","Concierge__enable_concierge","Cloud__enable_cloud_shell_fte_user_flow","BookNav__enable_tenant_cache_key","MiscFeatureFlags__enable_firebase_utm","Profiles__enable_profile_collections","Analytics__enable_all_tenant_analytics","MiscFeatureFlags__enable_explain_this_code","EngEduTelemetry__enable_engedu_telemetry","Cloud__enable_free_trial_server_call"],null,null,"AIzaSyBLEMok-5suZ67qRPzx0qUtbnLmyT_kCVE","https://developerscontentserving-pa.googleapis.com","AIzaSyCM4QpTRSqP5qI4Dvjt4OAScIN8sOUlO-k","https://developerscontentsearch-pa.googleapis.com",2,4,null,"https://developerprofiles-pa.googleapis.com",[1,"developers","Google for Developers","developers.google.com",null,"developers-dot-devsite-v2-prod.appspot.com",null,null,[1,1,[1],null,null,null,null,null,null,null,null,[1],null,null,null,null,null,null,[1],[1,null,null,[1,20],"/recommendations/information"],null,null,null,[1,1,1],[1,1,null,1,1]],null,[null,null,null,null,null,null,"/images/lockup-new.svg","/images/touchicon-180-new.png",null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,"/images/lockup-dark-theme-new.svg",[]],[],null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,[6,1,14,15,20,22,23,29,32,36],null,[[null,null,null,[3,7,10,2,39,17,4,32,24,11,12,13,34,15,25],null,null,[1,[["docType","Choose a content type",[["Tutorial",null,null,null,null,null,null,null,null,"Tutorial"],["Guide",null,null,null,null,null,null,null,null,"Guide"],["Sample",null,null,null,null,null,null,null,null,"Sample"]]],["product","Choose a product",[["Android",null,null,null,null,null,null,null,null,"Android"],["ARCore",null,null,null,null,null,null,null,null,"ARCore"],["ChromeOS",null,null,null,null,null,null,null,null,"ChromeOS"],["Firebase",null,null,null,null,null,null,null,null,"Firebase"],["Flutter",null,null,null,null,null,null,null,null,"Flutter"],["Assistant",null,null,null,null,null,null,null,null,"Google Assistant"],["GoogleCloud",null,null,null,null,null,null,null,null,"Google Cloud"],["GoogleMapsPlatform",null,null,null,null,null,null,null,null,"Google Maps Platform"],["GooglePay",null,null,null,null,null,null,null,null,"Google Pay & Google Wallet"],["GooglePlay",null,null,null,null,null,null,null,null,"Google Play"],["Tensorflow",null,null,null,null,null,null,null,null,"TensorFlow"]]],["category","Choose a topic",[["AiAndMachineLearning",null,null,null,null,null,null,null,null,"AI and Machine Learning"],["Data",null,null,null,null,null,null,null,null,"Data"],["Enterprise",null,null,null,null,null,null,null,null,"Enterprise"],["Gaming",null,null,null,null,null,null,null,null,"Gaming"],["Mobile",null,null,null,null,null,null,null,null,"Mobile"],["Web",null,null,null,null,null,null,null,null,"Web"]]]]]],[1,1],null,1],[[["UA-24532603-1"],["UA-22084204-5"],null,null,["UA-24532603-5"],null,null,[["G-272J68FCRF"],null,null,[["G-272J68FCRF",2]]],[["UA-24532603-1",2]],null,[["UA-24532603-5",2]],null,1],[[1,1],[11,8],[12,9],[13,10],[6,5],[14,11],[4,3],[16,13],[5,4],[3,2],[15,12]],[[2,2],[1,1]]],null,4,null,null,null,null,null,null,null,null,null,null,null,null,null,"developers.devsite.google"]]')
      
      </script>
    
  </head>
  
  <body>
    



    <meta charset="UTF-8">
    <title>Backpropagation demo</title>
    <link href="css/7tPClckhfMqx.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="css/lyFGJAff2iSE.css">
    <script src="js/X1ZpE2knmQQr.js" charset="utf-8"></script>
    <script src="js/PTr3COWozeDa.js" charset="utf-8"></script>
    <devsite-mathjax config="TeX-AMS-MML_HTMLorMML"></devsite-mathjax>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: {extensions: ["color.js"]},
        displayAlign: "left"
      });
    </script>


<div id="container">
  <div id="sections">
    <div style="display:none">
      $$
      \definecolor{input}{RGB}{66, 133, 244}
      \definecolor{output}{RGB}{219, 68, 55}
      \definecolor{dinput}{RGB}{244, 180, 0}
      \definecolor{doutput}{RGB}{15, 157, 88}
      \definecolor{dweight}{RGB}{102, 0, 255}
      $$
    </div>
    <section>
      <h2 id="backpropagation-algorithm" data-text="Backpropagation algorithm" tabindex="-1">Backpropagation algorithm</h2>
        The backpropagation algorithm is essential for training large neural networks
        quickly. This article explains how the algorithm works.<br><br>
        Please scroll down...
    </section>
    <section>
      <h2 id="simple-neural-network" data-text="Simple neural network" tabindex="-1">Simple neural network</h2>
        On the right, you see a neural network with one input, one output node
        and two hidden layers of two nodes each.
        <br><br>
        Nodes in neighboring layers are connected with weights \(w_{ij}\), which are the network
        parameters.
    </section>
    <section>
      <h2 id="activation-function" data-text="Activation function" tabindex="-1">Activation function</h2>
        Each node has a total input \(\color{input}x\), an activation function \(f(\color{input}x\color{black})\)
        and an output \(\color{output}y\color{black}=f(\color{input}x\color{black})\).
        \(f(\color{input}x\color{black})\) has to be a non-linear function, otherwise the neural network will only
        be able to learn linear models.
        <br><br>
        A commonly used activation function is
        the <a href="https://wikipedia.org/wiki/Sigmoid_function">Sigmoid function</a>:
          \(f(\color{input}x\color{black}) = \frac{1}{1+e^{-\color{input}x}}\).
    </section>
    <section>
      <h2 id="error-function" data-text="Error function" tabindex="-1">Error function</h2>
      The goal is to learn the weights of the network automatically from data such that the predicted output \(\color{output}y_{output}\)
      is close to the target \(\color{output}y_{target}\) for all inputs \(\color{input}x_{input}\). <br> <br>

      To measure how far we are from the goal, we use an error function \(E\).
      A commonly used error function is \(E(\color{output}y_{output}\color{black},\color{output}y_{target}\color{black}) = \frac{1}{2}(\color{output}y_{output}\color{black} - \color{output}y_{target}\color{black})^2 \).
    </section>
    <section>
      <h2 id="forward-propagation" data-text="Forward propagation" tabindex="-1">Forward propagation</h2>
        We begin by taking an input example \((\color{input}x_{input}\color{black},\color{output}y_{target}\color{black})\) and updating the input layer of the network.<br><br>
        For consistency, we consider the input to be like any other node but without an activation function so its output is equal to its input, i.e. \( \color{output}y_1 \color{black} = \color{input} x_{input} \).
    </section>
    <section>
      <h2 id="forward-propagation_1" data-text="Forward propagation" tabindex="-1">Forward propagation</h2>
        Now, we update the first hidden layer. We take the output \(\color{output}y\) of the nodes in the previous layer
        and use the weights to compute the input \(\color{input}x\) of the nodes in the next layer.
        <div class="formula Input"><span>$$ \color{input} x_j \color{black} = $$</span><span>$$ \sum_{i\in in(j)} w_{ij}\color{output} y_i\color{black} +b_j$$</span></div>
    </section>
    <section>
      <h2 id="forward-propagation_2" data-text="Forward propagation" tabindex="-1">Forward propagation</h2>
        Then we update the output of the nodes in the first hidden layer.
        For this we use the activation function, \( f(x) \).
        <div class="formula Output">$$ \color{output} y \color{black} = f(\color{input} x \color{black})$$</div>
    </section>
    <section data-num-sec="3">
      <h2 id="forward-propagation_3" data-text="Forward propagation" tabindex="-1">Forward propagation</h2>
        Using these 2 formulas we propagate for the rest of the network and get the final output of the network.
        <div class="formula Output">$$ \color{output} y \color{black} = f(\color{input} x \color{black})$$</div>
        <div class="formula Input"><span>$$ \color{input} x_j \color{black} = $$</span><span>$$ \sum_{i\in in(j)} w_{ij}\color{output} y_i \color{black} + b_j$$</span></div>
    </section>
    <section>
      <h2 id="error-derivative" data-text="Error derivative" tabindex="-1">Error derivative</h2>
        The backpropagation algorithm decides how much to
        update each weight of the network after comparing the predicted output with the desired output for a particular example.
        For this, we need to compute how the error changes
        with respect to each weight \(\color{dweight}\frac{dE}{dw_{ij}}\). <br>
        Once we have the error derivatives, we can update the weights using a simple update rule:
        <div class="formula">$$w_{ij} = w_{ij} - \alpha \color{dweight}\frac{dE}{dw_{ij}}$$</div>
        where \(\alpha\) is a positive constant, referred to as the learning rate, which we need to fine-tune empirically. <br> <br>
        <div style="font-size: 13px">
        [Note] The update rule is very simple: if the error goes down when the weight increases (\(\color{dweight}\frac{dE}{dw_{ij}}\color{black} &lt; 0\)),
        then increase the weight, otherwise if the error goes up when the weight increases (\(\color{dweight}\frac{dE}{dw_{ij}} \color{black} &gt; 0\)),
        then decrease the weight.
        </div>
    </section>
    <section>
      <h2 id="additional-derivatives" data-text="Additional derivatives" tabindex="-1">Additional derivatives</h2>
        To help compute \(\color{dweight}\frac{dE}{dw_{ij}}\), we additionally store for each node two more derivatives:
        how the error changes with:
        <ul>
          <li>the total input of the node \(\color{dinput}\frac{dE}{dx}\) and</li>
          <li>the output of the node \(\color{doutput}\frac{dE}{dy}\).</li>
        </ul>
    </section>
    <section>
      <h2 id="back-propagation" data-text="Back propagation" tabindex="-1">Back propagation</h2>
        Let's begin backpropagating the error derivatives.
        Since we have the predicted output of this particular input example, we can compute how the error changes with that output.
        Given our error function \(E = \frac{1}{2}(\color{output}y_{output}\color{black} - \color{output}y_{target}\color{black})^2\) we have:
        <div class="formula DerOutput">$$ \color{doutput} \frac{\partial E}{\partial y_{output}} \color{black} = \color{output} y_{output} \color{black} - \color{output} y_{target}$$</div>
    </section>
    <section>
      <h2 id="back-propagation_1" data-text="Back propagation" tabindex="-1">Back propagation</h2>
        Now that we have \(\color{doutput} \frac{dE}{dy}\) we can get \(\color{dinput}\frac{dE}{dx}\) using the chain rule.
        <div class="formula DerInput">$$\color{dinput} \frac{\partial E}{\partial x} \color{black} = \frac{dy}{dx}\color{doutput}\frac{\partial E}{\partial y} \color{black} = \frac{d}{dx}f(\color{input}x\color{black})\color{doutput}\frac{\partial E}{\partial y}$$</div>
        where \(\frac{d}{dx}f(\color{input}x\color{black}) = f(\color{input}x\color{black})(1 - f(\color{input}x\color{black}))\) when
        \(f(\color{input}x\color{black})\) is the Sigmoid activation function.
    </section>
    <section>
      <h2 id="back-propagation_2" data-text="Back propagation" tabindex="-1">Back propagation</h2>
      As soon as we have the error derivative with respect to the total input of a node,
      we can get the error derivative with respect to the weights coming into that node.
      <div class="formula DerWeight">$$\color{dweight} \frac{\partial E}{\partial w_{ij}} \color{black} = \frac{\partial x_j}{\partial w_{ij}} \color{dinput}\frac{\partial E}{\partial x_j} \color{black} = \color{output}y_i \color{dinput} \frac{\partial E}{\partial x_j}$$</div>
    </section>
    <section>
      <h2 id="back-propagation_3" data-text="Back propagation" tabindex="-1">Back propagation</h2>
        And using the chain rule, we can also get \(\frac{dE}{dy}\) from the previous layer. We have made a full circle.
        <div class="formula DerOutput">$$ \color{doutput} \frac{\partial E}{\partial y_i} \color{black} = \sum_{j\in out(i)} \frac{\partial x_j}{\partial y_i} \color{dinput} \frac{\partial E}{\partial x_j} \color{black} = \sum_{j\in out(i)} w_{ij} \color{dinput} \frac{\partial E}{\partial x_j}$$</div>
    </section>
    <section data-num-sec="4">
      <h2 id="back-propagation_4" data-text="Back propagation" tabindex="-1">Back propagation</h2>
      All that is left to do is repeat the previous three formulas until we have computed all the error derivatives.
    </section>
    <section>
      <h2 id="the-end." data-text="The end." tabindex="-1">The end.</h2>
    </section>
  </div>
  <div id="vis">
    <svg id="mainsvg" width="300" height="700">
    <defs>
      <marker id="markerCircle" markerWidth="8" markerHeight="8" refX="5" refY="5">
        <circle cx="3" cy="3" r="3" style="stroke: none; fill:rgb(166, 166, 166);"></circle>
      </marker>
      <marker id="markerArrow" markerWidth="13" markerHeight="13" refX="2" refY="6" orient="auto">
      <path d="M2,2 L2,11 L10,6 L2,2" style="fill: rgb(166, 166, 166);"></path>
      </marker>
    </defs>
    </svg>
    <div id="loader">
      <div class="atebits-loader"></div>
      <div style="margin-left: 20px; float:right;">Computing...</div>
    </div>
  </div>
  <div id="extra-space"></div>
</div>
<script src="js/adfGW7b8EEfI.js"></script>
<script src="js/ElIMwTHpiMyF.js"></script>


  
</body></html>